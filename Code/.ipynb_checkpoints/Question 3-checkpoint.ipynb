{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import string\n",
    "import nltk \n",
    "import re\n",
    "from nltk import word_tokenize \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from nltk.stem import WordNetLemmatizer,SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt \n",
    "from nltk.collocations import BigramCollocationFinder \n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the text from text file and make a dataframe\n",
    "l_col1 = []\n",
    "with open(\"data/text.txt\",\"r\") as file:\n",
    "    lines = file.readlines()\n",
    "    for ind, line in enumerate(lines[:-1]):\n",
    "        l_col1.append(line)\n",
    "df= pd.DataFrame({'sentences': l_col1})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply first round of text cleaning techniques\n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    #get rid of non english charectors\n",
    "    text = re.sub(\"([^\\x00-\\x7F])+\",\"\",text)\n",
    "    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the updated text\n",
    "df['CleanedSentences'] = pd.DataFrame(df['sentences'].apply(round1))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "#tokenizing and removing stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['tokenizeSentences'] = None\n",
    "for i in df.index:\n",
    "    word_tokens = word_tokenize(df['CleanedSentences'][i])\n",
    "    df['CleanedSentences'][i] = [w for w in word_tokens if not w in stop_words] \n",
    "    \n",
    "    \n",
    "    Deleted_list = [',']\n",
    "    \n",
    "    word_tokens = word_tokenize(df['sentences'][i])\n",
    "    df['tokenizeSentences'][i] = [w for w in word_tokens if not w in Deleted_list]\n",
    "    \n",
    "    \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  using nltk WordNetLemmatizer function to normalize the tweets\n",
    "# Text Normalization -reducing terms to their base word(like runs, run, running, ran to base word run)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['CleanedSentences'] = df['CleanedSentences'].apply(lambda x: [lemmatizer.lemmatize(i) for i in x])\n",
    "\n",
    "\n",
    "for i in range(len(df['CleanedSentences'])):\n",
    "    df['CleanedSentences'][i] = ' '.join(df['CleanedSentences'][i])    \n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new column for tokens\n",
    "df['tokenizedWords'] = df['CleanedSentences'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of words in each doc\n",
    "word_count_list = np.zeros(df.shape[0], dtype=np.int)\n",
    "doc_list = df['CleanedSentences'].values\n",
    "for i in df.index:\n",
    "    word_count_list[i] = len(doc_list[i].split())\n",
    "\n",
    "# add the word count as a new column\n",
    "df['word_count'] = word_count_list\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count_Line(df):\n",
    "    temp = []\n",
    "    for l in df.CleanedSentences.values:\n",
    "        counts = dict()\n",
    "        words = l.split()\n",
    "\n",
    "        for word in words:\n",
    "            if word in counts:\n",
    "                counts[word] += 1\n",
    "            else:\n",
    "                counts[word] = 1\n",
    "        temp.append(counts)  \n",
    "    df['word_count_line']=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_Line(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_prob(df,col,word):\n",
    "    i = 0\n",
    "    p = 0\n",
    "    for my_dict in df[col]:\n",
    "        for k,v in my_dict.items():\n",
    "            if k == word:\n",
    "                p = (v/df['word_count'][i])\n",
    "        print(\"Probability of the '{0}' occuring in line {1} is {2}\".format(word,i,p))\n",
    "        p = 0\n",
    "        i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_prob(df,'word_count_line','data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Above function shows the probability of word 'data' appearing in each line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new df to store words\n",
    "columns=['text_index', 'word']\n",
    "word_df_stopwords = pd.DataFrame(columns=columns)\n",
    "# convert df to array to improve loop time performance\n",
    "words = df['tokenizeSentences'].values\n",
    "    \n",
    "for i in df.index:\n",
    "    temp = [[str(i), w] for w in words[i]]\n",
    "    temp_df = pd.DataFrame(temp, columns=columns)\n",
    "    # add 1 row for each word\n",
    "    word_df_stopwords = word_df_stopwords.append(temp_df, ignore_index=True)\n",
    "    \n",
    "word_df_stopwords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new df to store words\n",
    "columns=['text_index', 'word']\n",
    "word_df = pd.DataFrame(columns=columns)\n",
    "# convert df to array to improve loop time performance\n",
    "words = df['tokenizedWords'].values\n",
    "    \n",
    "for i in df.index:\n",
    "    temp = [[str(i), w] for w in words[i]]\n",
    "    temp_df = pd.DataFrame(temp, columns=columns)\n",
    "    # add 1 row for each word\n",
    "    word_df = word_df.append(temp_df, ignore_index=True)\n",
    "    \n",
    "word_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find the most frequent values in a column and their frequencies\n",
    "from collections import Counter\n",
    "def most_commons(df, cl_name):\n",
    "    return pd.DataFrame(Counter(df[cl_name]).most_common(), columns=[cl_name, 'frequency'])\n",
    "\n",
    "# top most frequently mentioned named entities\n",
    "(most_commons(word_df, 'word')).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table above shows the top 5 words appear the most in the text with their respective frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def barchart_words(word_df,title):\n",
    "    \n",
    "    temp_word = word_df.word.tolist()\n",
    "    temp_freq = word_df.frequency.tolist()\n",
    "\n",
    "    y_pos = np.arange(len(temp_word))\n",
    "\n",
    "    plt.barh(y_pos, temp_freq, align='center', alpha=0.5 , \n",
    "             color=['black', 'red', 'green', 'blue', 'cyan','magenta','yellow','Purple','orange','Brown'])\n",
    "    plt.yticks(y_pos, temp_word)\n",
    "    plt.xlabel('count')\n",
    "    plt.title(title)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barchart_words((most_commons(word_df, 'word')).head(),'Top 5 words repeated in this text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot word cloud of a given text\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "def plot_word_cloud(df, colormap='bwr', title=None, width=1000, height=800):\n",
    "    # set the stop words\n",
    "    stopwords = set(STOPWORDS)\n",
    "    Additional_stop_words = ['']\n",
    "    words = (' '.join(df['word'])).lower()\n",
    "    for s in  Additional_stop_words:\n",
    "        stopwords.add(s)\n",
    "        \n",
    "    # generate WordCloud\n",
    "    if type(words)==str:\n",
    "        wc = WordCloud(background_color=\"white\", stopwords=stopwords, \n",
    "                       width=width, height=height, colormap=colormap).generate(words)\n",
    "    # for simplicity, I assume that if words is not an string, it is a dictionary of word frequencies\n",
    "    else:\n",
    "        wc = WordCloud(background_color=\"white\", stopwords=stopwords, \n",
    "                       width=width, height=height, colormap=colormap).generate_from_frequencies(words)\n",
    "\n",
    "    # show the WordCloud\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    if title:\n",
    "        plt.title(title, fontsize=25, y=1.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_word_cloud(word_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordcloud above shows the popularity distributions of each word in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = most_commons(word_df, 'word')\n",
    "temp_df2 = most_commons(word_df_stopwords, 'word')\n",
    "\n",
    "print(\"There are'{0}' words (without stop words) or '{1}' words (with stop words) distinct words in text\".format(temp_df.shape[0],temp_df2.shape[0]))\n",
    "\n",
    "temp_df.head(153)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. There are 153 words (without the stop words) or 202 words (with stop words) distinct words in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = most_commons(word_df, 'word')\n",
    "for i in temp.index:\n",
    "    print(\"Probability of the '{0}' occuring in text {1}\".format(temp.word[i],temp.frequency[i]/temp.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNGrams(wordlist, n):\n",
    "    ng = []\n",
    "    for i in range(len(wordlist)-(n-1)):\n",
    "        ng.append(wordlist[i:i+n])\n",
    "    return ng\n",
    "\n",
    "def NGramsFlatList(df):\n",
    "    ng = []\n",
    "    flat_list = []\n",
    "    temp = []\n",
    "    for i in df.index:\n",
    "        temp.append(df['tokenizedWords'][i])\n",
    "    for sublist in temp:\n",
    "        for item in sublist:\n",
    "            flat_list.append(item)\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twoWords(df,word_df,First,Second):\n",
    "    unifl = NGramsFlatList(df)\n",
    "    bifl = getNGrams(unifl,2)\n",
    "    noOccurance = bifl.count([First,Second])\n",
    "    FirstWordOccurance = ((word_df[word_df['word'] == First]).count()).word\n",
    "    print(\"Probability of word '{0}' occuring after word '{1}' is: {2}\".format(First,Second,(noOccurance/FirstWordOccurance)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twoWords(df,word_df,'data','analytics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Probability of word 'data' occuring after word 'analytics' is: 0.3333333333333333\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
